{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Letter Generation Pipeline\n",
    "\n",
    "A LangGraph-based pipeline for generating client letters using Google Gemini with human-in-the-loop feedback.\n",
    "\n",
    "## Features\n",
    "\n",
    "- **Dual Template System**: Combines general guidance + specific letter type templates\n",
    "- **Looping Workflow**: Iteratively improve templates based on feedback\n",
    "- **Human-in-the-Loop**: Review evaluation results and provide feedback\n",
    "- **Session Tracking**: All iterations and data saved for review\n",
    "- **Hallucination Detection**: Automatic evaluation of generated letters\n",
    "\n",
    "## Quick Start\n",
    "\n",
    "1. Set up your `.env` file with `GOOGLE_API_KEY`\n",
    "2. Run Cell 1 to initialize\n",
    "3. Run Cell 8 for interactive letter generation\n",
    "4. Or run Cell 10 for a quick test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Setup\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Install dependencies\n",
    "def install_packages():\n",
    "    packages = ['google-genai', 'langgraph', 'python-docx', 'pdfplumber', 'python-dotenv']\n",
    "    for package in packages:\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', package, '-q'])\n",
    "\n",
    "install_packages()\n",
    "\n",
    "# Import required libraries\n",
    "from typing import Dict, List, Optional, TypedDict\n",
    "import re\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Configure Google API\n",
    "GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')\n",
    "MODEL = os.getenv('MODEL', 'gemini-2.5-flash')  # Updated default\n",
    "TEMPERATURE_DRAFT = 0.25\n",
    "# Initialize client\n",
    "client = genai.Client(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "print(f\"Model configured: {MODEL}\")\n",
    "print(f\"API Key loaded: {'Yes' if GOOGLE_API_KEY else 'No'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Helper Functions\n",
    "import docx\n",
    "import pdfplumber\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "import hashlib\n",
    "from typing import Any\n",
    "\n",
    "# Global metrics tracking\n",
    "llm_metrics = {\n",
    "    'total_api_calls': 0,\n",
    "    'total_tokens_in': 0,\n",
    "    'total_tokens_out': 0,\n",
    "    'total_latency': 0.0,\n",
    "    'api_calls_by_type': {}\n",
    "}\n",
    "\n",
    "def reset_llm_metrics():\n",
    "    \"\"\"Reset LLM metrics for new iteration.\"\"\"\n",
    "    global llm_metrics\n",
    "    llm_metrics = {\n",
    "        'total_api_calls': 0,\n",
    "        'total_tokens_in': 0,\n",
    "        'total_tokens_out': 0,\n",
    "        'total_latency': 0.0,\n",
    "        'api_calls_by_type': {}\n",
    "    }\n",
    "\n",
    "def get_llm_metrics_snapshot():\n",
    "    \"\"\"Get current LLM metrics snapshot.\"\"\"\n",
    "    return {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'total_api_calls': llm_metrics['total_api_calls'],\n",
    "        'total_tokens_in': llm_metrics['total_tokens_in'],\n",
    "        'total_tokens_out': llm_metrics['total_tokens_out'],\n",
    "        'total_latency_seconds': round(llm_metrics['total_latency'], 2),\n",
    "        'average_latency_seconds': round(llm_metrics['total_latency'] / llm_metrics['total_api_calls'], 2) if llm_metrics['total_api_calls'] > 0 else 0,\n",
    "        'api_calls_breakdown': dict(llm_metrics['api_calls_by_type'])\n",
    "    }\n",
    "\n",
    "def read_file_smart(path: str) -> str:\n",
    "    \"\"\"Read file content from various formats.\"\"\"\n",
    "    path_obj = Path(path)\n",
    "    \n",
    "    if path_obj.suffix in ['.txt', '.md']:\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            return f.read()\n",
    "    \n",
    "    elif path_obj.suffix == '.docx':\n",
    "        # Read docx file\n",
    "        doc = docx.Document(path)\n",
    "        content = []\n",
    "        for para in doc.paragraphs:\n",
    "            if para.text.strip():\n",
    "                content.append(para.text)\n",
    "        return '\\n'.join(content)\n",
    "    \n",
    "    elif path_obj.suffix == '.pdf':\n",
    "        # Read pdf file\n",
    "        content = []\n",
    "        with pdfplumber.open(path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                text = page.extract_text()\n",
    "                if text:\n",
    "                    content.append(text)\n",
    "        return '\\n'.join(content)\n",
    "    \n",
    "    else:\n",
    "        return f\"[Unsupported format: {path_obj.suffix}]\"\n",
    "\n",
    "\n",
    "def read_case_folder(case_folder: str) -> str:\n",
    "    \"\"\"Read all files in a case folder and aggregate content.\"\"\"\n",
    "    folder_path = Path(case_folder)\n",
    "    if not folder_path.exists():\n",
    "        raise ValueError(f\"Case folder not found: {case_folder}\")\n",
    "    \n",
    "    all_content = []\n",
    "    \n",
    "    # Read all docx and pdf files in the folder\n",
    "    for file_path in sorted(folder_path.glob(\"*\")):\n",
    "        if file_path.suffix in ['.docx', '.pdf']:\n",
    "            content = read_file_smart(str(file_path))\n",
    "            all_content.append(f\"\\n--- File: {file_path.name} ---\\n{content}\")\n",
    "    \n",
    "    if not all_content:\n",
    "        raise ValueError(f\"No case files found in: {case_folder}\")\n",
    "    \n",
    "    return '\\n\\n'.join(all_content)\n",
    "\n",
    "\n",
    "def read_general_template() -> str:\n",
    "    \"\"\"Read the general guidance template.\"\"\"\n",
    "    templates_dir = Path(\"data/templates\")\n",
    "    general_path = templates_dir / \"guidance.md\"\n",
    "    \n",
    "    if not general_path.exists():\n",
    "        raise ValueError(f\"General guidance template not found: {general_path}\")\n",
    "    \n",
    "    return read_file_smart(str(general_path))\n",
    "\n",
    "\n",
    "def read_specific_template(template_type: str = \"annual_review\") -> str:\n",
    "    \"\"\"Read the specific template for the given type.\"\"\"\n",
    "    templates_dir = Path(\"data/templates\")\n",
    "    specific_path = templates_dir / f\"{template_type}_guidance.md\"\n",
    "    \n",
    "    if not specific_path.exists():\n",
    "        # Return empty string if specific template doesn't exist\n",
    "        print(f\"⚠️  Specific template not found: {specific_path}\")\n",
    "        return \"\"\n",
    "    \n",
    "    return read_file_smart(str(specific_path))\n",
    "\n",
    "\n",
    "def combine_templates(general_template: str, specific_template: str) -> str:\n",
    "    \"\"\"Combine general and specific templates for letter generation.\"\"\"\n",
    "    if specific_template:\n",
    "        return f\"{general_template}\\n\\n--- SPECIFIC TEMPLATE GUIDANCE ---\\n\\n{specific_template}\"\n",
    "    return general_template\n",
    "\n",
    "\n",
    "def create_session_folder(client_name: str) -> Path:\n",
    "    \"\"\"Create a folder for this session's data.\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    session_dir = Path(f\"sessions/{client_name}_{timestamp}\")\n",
    "    session_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Create templates subdirectory\n",
    "    templates_dir = session_dir / \"templates\"\n",
    "    templates_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    return session_dir\n",
    "\n",
    "\n",
    "def get_template_hash(template_content: str) -> str:\n",
    "    \"\"\"Generate a hash of template content to detect changes.\"\"\"\n",
    "    return hashlib.md5(template_content.encode()).hexdigest()\n",
    "\n",
    "\n",
    "def save_template_if_changed(session_dir: Path, iteration: int, template_type: str, \n",
    "                           current_content: str, previous_hash: str = None) -> tuple[bool, str, str]:\n",
    "    \"\"\"Save template if it has changed from previous iteration.\n",
    "    \n",
    "    Returns: (changed, new_hash, file_path)\n",
    "    \"\"\"\n",
    "    current_hash = get_template_hash(current_content)\n",
    "    changed = current_hash != previous_hash if previous_hash else True\n",
    "    \n",
    "    if changed:\n",
    "        # Save template\n",
    "        templates_dir = session_dir / \"templates\"\n",
    "        file_path = templates_dir / f\"iteration_{iteration}_{template_type}.md\"\n",
    "        with open(file_path, \"w\") as f:\n",
    "            f.write(current_content)\n",
    "        return True, current_hash, str(file_path.relative_to(session_dir))\n",
    "    \n",
    "    return False, current_hash, None\n",
    "\n",
    "\n",
    "def save_iteration_data(session_dir: Path, iteration: int, state: Dict) -> None:\n",
    "    \"\"\"Save data from current iteration to file, including templates and LLM metrics.\"\"\"\n",
    "    # Get previous template hashes if available\n",
    "    prev_general_hash = state.get('general_template_hash')\n",
    "    prev_specific_hash = state.get('specific_template_hash')\n",
    "    \n",
    "    # Save templates and check if they changed\n",
    "    general_changed, general_hash, general_path = save_template_if_changed(\n",
    "        session_dir, iteration, \"general\", \n",
    "        state.get(\"general_template\", \"\"), prev_general_hash\n",
    "    )\n",
    "    \n",
    "    specific_changed, specific_hash, specific_path = save_template_if_changed(\n",
    "        session_dir, iteration, \"specific\",\n",
    "        state.get(\"specific_template\", \"\"), prev_specific_hash\n",
    "    )\n",
    "    \n",
    "    # Update state with new hashes for next iteration\n",
    "    state['general_template_hash'] = general_hash\n",
    "    state['specific_template_hash'] = specific_hash\n",
    "    \n",
    "    # Get LLM metrics snapshot\n",
    "    llm_metrics_snapshot = get_llm_metrics_snapshot()\n",
    "    \n",
    "    # Build iteration data\n",
    "    data = {\n",
    "        \"iteration\": iteration,\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"letter_length\": len(state.get(\"letter\", \"\")),\n",
    "        \"hallucinations_count\": len(state.get(\"hallucinations\", [])),\n",
    "        \"hallucinations\": state.get(\"hallucinations\", [])[:10],  # Save first 10\n",
    "        \"evaluation_notes\": state.get(\"evaluation_notes\", \"\"),\n",
    "        \"user_decision\": state.get(\"user_decision\", \"\"),\n",
    "        \"user_feedback\": state.get(\"user_feedback\", \"\"),\n",
    "        \"template_updated\": {\n",
    "            \"general\": state.get(\"general_template_updated\", False),\n",
    "            \"specific\": state.get(\"specific_template_updated\", False)\n",
    "        },\n",
    "        \"template_files\": {\n",
    "            \"general\": general_path if general_changed else f\"Same as iteration {iteration-1}\" if iteration > 1 else general_path,\n",
    "            \"specific\": specific_path if specific_changed else f\"Same as iteration {iteration-1}\" if iteration > 1 else specific_path\n",
    "        },\n",
    "        \"template_changed_this_iteration\": {\n",
    "            \"general\": general_changed,\n",
    "            \"specific\": specific_changed\n",
    "        },\n",
    "        # Add complete evaluation data\n",
    "        \"evaluation\": state.get(\"evaluation\", {}),\n",
    "        # Add LLM performance metrics\n",
    "        \"llm_metrics\": llm_metrics_snapshot\n",
    "    }\n",
    "    \n",
    "    # Save iteration data\n",
    "    with open(session_dir / f\"iteration_{iteration}.json\", \"w\") as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "    \n",
    "    # Save letter HTML\n",
    "    letter_path = session_dir / f\"letter_iteration_{iteration}.html\"\n",
    "    with open(letter_path, \"w\") as f:\n",
    "        f.write(state.get(\"letter\", \"\"))\n",
    "    \n",
    "    # Save LLM metrics as separate file for easy analysis\n",
    "    metrics_path = session_dir / f\"llm_metrics_iteration_{iteration}.json\"\n",
    "    with open(metrics_path, \"w\") as f:\n",
    "        json.dump(llm_metrics_snapshot, f, indent=2)\n",
    "    \n",
    "    print(f\"💾 Saved iteration {iteration} data to {session_dir.name}\")\n",
    "    if general_changed or specific_changed:\n",
    "        changed_templates = []\n",
    "        if general_changed:\n",
    "            changed_templates.append(\"general\")\n",
    "        if specific_changed:\n",
    "            changed_templates.append(\"specific\")\n",
    "        print(f\"📝 Saved updated templates: {', '.join(changed_templates)}\")\n",
    "    print(f\"📊 LLM Metrics: {llm_metrics_snapshot['total_api_calls']} API calls, {llm_metrics_snapshot['total_latency_seconds']}s total latency\")\n",
    "\n",
    "\n",
    "def save_session_summary(session_dir: Path, final_state: Dict) -> None:\n",
    "    \"\"\"Save a summary of the entire session.\"\"\"\n",
    "    iterations_data = final_state.get(\"iterations_data\", [])\n",
    "    \n",
    "    summary = f\"\"\"Session Summary\n",
    "===============\n",
    "Client: {final_state.get('client_name', 'Unknown')}\n",
    "Template Type: {final_state.get('template_type', 'Unknown')}\n",
    "Total Iterations: {final_state.get('iteration', 0)}\n",
    "Session Folder: {session_dir}\n",
    "\n",
    "Template Evolution:\n",
    "\"\"\"\n",
    "    \n",
    "    # Track template changes\n",
    "    template_changes = []\n",
    "    for i in range(1, final_state.get('iteration', 0) + 1):\n",
    "        iter_file = session_dir / f\"iteration_{i}.json\"\n",
    "        if iter_file.exists():\n",
    "            with open(iter_file, 'r') as f:\n",
    "                iter_data = json.load(f)\n",
    "                if iter_data.get('template_changed_this_iteration', {}).get('general') or \\\n",
    "                   iter_data.get('template_changed_this_iteration', {}).get('specific'):\n",
    "                    changes = []\n",
    "                    if iter_data.get('template_changed_this_iteration', {}).get('general'):\n",
    "                        changes.append(\"general\")\n",
    "                    if iter_data.get('template_changed_this_iteration', {}).get('specific'):\n",
    "                        changes.append(\"specific\")\n",
    "                    template_changes.append(f\"  - Iteration {i}: Updated {', '.join(changes)} template(s)\")\n",
    "    \n",
    "    if template_changes:\n",
    "        summary += \"\\n\".join(template_changes) + \"\\n\"\n",
    "    else:\n",
    "        summary += \"  - No template changes during session\\n\"\n",
    "    \n",
    "    # Add LLM metrics summary\n",
    "    summary += \"\\nLLM Performance Metrics:\\n\"\n",
    "    total_api_calls = 0\n",
    "    total_latency = 0.0\n",
    "    \n",
    "    for i in range(1, final_state.get('iteration', 0) + 1):\n",
    "        metrics_file = session_dir / f\"llm_metrics_iteration_{i}.json\"\n",
    "        if metrics_file.exists():\n",
    "            with open(metrics_file, 'r') as f:\n",
    "                metrics = json.load(f)\n",
    "                api_calls = metrics.get('total_api_calls', 0)\n",
    "                latency = metrics.get('total_latency_seconds', 0)\n",
    "                total_api_calls += api_calls\n",
    "                total_latency += latency\n",
    "                summary += f\"  - Iteration {i}: {api_calls} API calls, {latency}s latency\\n\"\n",
    "    \n",
    "    summary += f\"\\nTotal Session Metrics:\\n\"\n",
    "    summary += f\"  - Total API Calls: {total_api_calls}\\n\"\n",
    "    summary += f\"  - Total Latency: {total_latency:.2f}s\\n\"\n",
    "    summary += f\"  - Average Latency per Call: {(total_latency/total_api_calls):.2f}s\\n\" if total_api_calls > 0 else \"\"\n",
    "    \n",
    "    summary += f\"\"\"\n",
    "Iteration Details:\n",
    "\"\"\"\n",
    "    \n",
    "    for i, data in enumerate(iterations_data, 1):\n",
    "        # Fix: Safely handle user_feedback that might be None\n",
    "        feedback = data.get('user_feedback') or 'None'\n",
    "        summary += f\"\"\"\n",
    "Iteration {i}:\n",
    "  - Hallucinations: {data.get('hallucinations_count', 0)}\n",
    "  - User Decision: {data.get('user_decision', 'N/A')}\n",
    "  - Feedback: {feedback[:100]}...\n",
    "\"\"\"\n",
    "    \n",
    "    summary += f\"\"\"\n",
    "Final Status: Letter {'Accepted' if final_state.get('user_decision') == 'accept' else 'In Progress'}\n",
    "Final Letter: {session_dir / f\"letter_iteration_{final_state.get('iteration', 0)}.html\"}\n",
    "Final Templates: {session_dir / \"templates\"}\n",
    "LLM Metrics: {session_dir / \"llm_metrics_iteration_*.json\"}\n",
    "\n",
    "All iteration files saved in: {session_dir}\n",
    "\"\"\"\n",
    "    \n",
    "    with open(session_dir / \"session_summary.txt\", \"w\") as f:\n",
    "        f.write(summary)\n",
    "    \n",
    "    print(f\"\\n📄 Session summary saved to: {session_dir / 'session_summary.txt'}\")\n",
    "\n",
    "\n",
    "def load_template_from_session(session_dir: Path, iteration: int, template_type: str) -> str:\n",
    "    \"\"\"Load a specific template version from a session.\"\"\"\n",
    "    # First try the exact iteration\n",
    "    template_path = session_dir / \"templates\" / f\"iteration_{iteration}_{template_type}.md\"\n",
    "    \n",
    "    if template_path.exists():\n",
    "        with open(template_path, 'r') as f:\n",
    "            return f.read()\n",
    "    \n",
    "    # If not found, look for the most recent version before this iteration\n",
    "    for i in range(iteration - 1, 0, -1):\n",
    "        template_path = session_dir / \"templates\" / f\"iteration_{i}_{template_type}.md\"\n",
    "        if template_path.exists():\n",
    "            with open(template_path, 'r') as f:\n",
    "                return f.read()\n",
    "    \n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def compare_templates(template1: str, template2: str) -> str:\n",
    "    \"\"\"Simple comparison of two templates showing additions/deletions.\"\"\"\n",
    "    lines1 = template1.splitlines()\n",
    "    lines2 = template2.splitlines()\n",
    "    \n",
    "    # Simple diff visualization\n",
    "    diff_output = []\n",
    "    \n",
    "    # This is a simplified diff - in production you might use difflib\n",
    "    if lines1 != lines2:\n",
    "        diff_output.append(\"Template differences detected:\")\n",
    "        diff_output.append(f\"  - Previous version: {len(lines1)} lines\")\n",
    "        diff_output.append(f\"  - New version: {len(lines2)} lines\")\n",
    "        diff_output.append(f\"  - Line difference: {len(lines2) - len(lines1)}\")\n",
    "    else:\n",
    "        diff_output.append(\"No changes detected in template\")\n",
    "    \n",
    "    return \"\\n\".join(diff_output)\n",
    "\n",
    "\n",
    "def strip_markdown(text: str) -> str:\n",
    "    \"\"\"Remove basic markdown syntax from text.\"\"\"\n",
    "    # Remove headers\n",
    "    text = re.sub(r'^#+\\s+', '', text, flags=re.MULTILINE)\n",
    "    # Remove bold/italic\n",
    "    text = re.sub(r'\\*{1,2}([^*]+)\\*{1,2}', r'\\1', text)\n",
    "    text = re.sub(r'_{1,2}([^_]+)_{1,2}', r'\\1', text)\n",
    "    # Remove code blocks\n",
    "    text = re.sub(r'```[^`]*```', '', text, flags=re.DOTALL)\n",
    "    text = re.sub(r'`([^`]+)`', r'\\1', text)\n",
    "    # Remove links\n",
    "    text = re.sub(r'\\[([^\\]]+)\\]\\([^)]+\\)', r'\\1', text)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def provider_llm(**kwargs) -> Any:\n",
    "    \"\"\"Create a Google Gemini completion with structured output support.\"\"\"\n",
    "    global llm_metrics\n",
    "    \n",
    "    # Extract messages, temperature, and response format\n",
    "    messages = kwargs.get('messages', [])\n",
    "    temperature = kwargs.get('temperature', 0.4)\n",
    "    response_format = kwargs.get('response_format')\n",
    "    system_prompt = kwargs.get('system_prompt', '')\n",
    "    call_type = kwargs.get('call_type', 'general')  # For categorizing API calls\n",
    "    \n",
    "    # Convert messages to Gemini format\n",
    "    if messages and messages[0]['role'] == 'user':\n",
    "        prompt = messages[0]['content']\n",
    "    else:\n",
    "        prompt = str(messages)\n",
    "    \n",
    "    # Add system prompt if provided\n",
    "    if system_prompt:\n",
    "        prompt = f\"{system_prompt}\\n\\n{prompt}\"\n",
    "    \n",
    "    # Estimate input tokens (rough approximation: 1 token ≈ 4 chars)\n",
    "    input_tokens = len(prompt) // 4\n",
    "    \n",
    "    print(f\"🤖 Calling Gemini API (temp={temperature}, structured={response_format is not None})...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Configure generation based on whether we want structured output\n",
    "        if response_format:\n",
    "            # Use JSON mode for structured output - NO MAX TOKEN LIMIT\n",
    "            response = client.models.generate_content(\n",
    "                model=MODEL,\n",
    "                contents=prompt,\n",
    "                config=types.GenerateContentConfig(\n",
    "                    temperature=temperature,\n",
    "                    candidate_count=1,\n",
    "                    response_mime_type=\"application/json\",\n",
    "                    response_schema=response_format\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            # Regular text generation - NO MAX TOKEN LIMIT\n",
    "            response = client.models.generate_content(\n",
    "                model=MODEL,\n",
    "                contents=prompt,\n",
    "                config=types.GenerateContentConfig(\n",
    "                    temperature=temperature,\n",
    "                    candidate_count=1,\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        # Check if response has candidates\n",
    "        if not response.candidates:\n",
    "            raise ValueError(\"No candidates in API response\")\n",
    "        \n",
    "        # Extract text from response\n",
    "        candidate = response.candidates[0]\n",
    "        \n",
    "        # Try different ways to extract content\n",
    "        result_text = None\n",
    "        \n",
    "        # Method 1: Try content.parts[0].text\n",
    "        if hasattr(candidate, 'content') and candidate.content:\n",
    "            if hasattr(candidate.content, 'parts') and candidate.content.parts:\n",
    "                if hasattr(candidate.content.parts[0], 'text'):\n",
    "                    result_text = candidate.content.parts[0].text\n",
    "                else:\n",
    "                    # Parts might be a list of dicts\n",
    "                    result_text = str(candidate.content.parts[0])\n",
    "        \n",
    "        # Method 2: Try direct text attribute\n",
    "        if not result_text and hasattr(candidate, 'text'):\n",
    "            result_text = candidate.text\n",
    "        \n",
    "        # Method 3: Try to convert to string\n",
    "        if not result_text:\n",
    "            result_text = str(candidate)\n",
    "        \n",
    "        if not result_text or result_text == \"None\":\n",
    "            raise ValueError(\"Could not extract text from API response\")\n",
    "        \n",
    "        # Calculate metrics\n",
    "        elapsed = time.time() - start_time\n",
    "        output_tokens = len(result_text) // 4  # Rough approximation\n",
    "        \n",
    "        # Update global metrics\n",
    "        llm_metrics['total_api_calls'] += 1\n",
    "        llm_metrics['total_tokens_in'] += input_tokens\n",
    "        llm_metrics['total_tokens_out'] += output_tokens\n",
    "        llm_metrics['total_latency'] += elapsed\n",
    "        llm_metrics['api_calls_by_type'][call_type] = llm_metrics['api_calls_by_type'].get(call_type, 0) + 1\n",
    "        \n",
    "        print(f\"✅ API call completed in {elapsed:.2f}s (in: ~{input_tokens} tokens, out: ~{output_tokens} tokens)\")\n",
    "        \n",
    "        # If structured output was requested, parse JSON\n",
    "        if response_format:\n",
    "            try:\n",
    "                # Clean the text - sometimes it has extra whitespace\n",
    "                result_text = result_text.strip()\n",
    "                return json.loads(result_text)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"⚠️  Failed to parse JSON response: {e}\")\n",
    "                print(f\"   Raw text: {result_text[:500]}...\")\n",
    "                # Try to extract JSON from the response\n",
    "                match = re.search(r'\\{.*\\}', result_text, re.DOTALL)\n",
    "                if match:\n",
    "                    return json.loads(match.group())\n",
    "                raise ValueError(f\"Invalid JSON response: {result_text[:200]}...\")\n",
    "        \n",
    "        return result_text\n",
    "        \n",
    "    except Exception as e:\n",
    "        elapsed = time.time() - start_time\n",
    "        # Still track failed calls\n",
    "        llm_metrics['total_api_calls'] += 1\n",
    "        llm_metrics['total_latency'] += elapsed\n",
    "        llm_metrics['api_calls_by_type'][f\"{call_type}_failed\"] = llm_metrics['api_calls_by_type'].get(f\"{call_type}_failed\", 0) + 1\n",
    "        \n",
    "        print(f\"❌ API call failed after {elapsed:.2f}s: {type(e).__name__}: {str(e)}\")\n",
    "        \n",
    "        # Log more details for debugging\n",
    "        if hasattr(e, 'response'):\n",
    "            print(f\"   Response status: {getattr(e.response, 'status_code', 'N/A')}\")\n",
    "        \n",
    "        # Re-raise the exception to prevent hanging\n",
    "        raise Exception(f\"Gemini API error: {str(e)}\")\n",
    "\n",
    "\n",
    "def test_api():\n",
    "    \"\"\"Test basic API connectivity.\"\"\"\n",
    "    print(\"🧪 Testing Gemini API connection...\")\n",
    "    try:\n",
    "        response = provider_llm(\n",
    "            messages=[{\"role\": \"user\", \"content\": \"Say 'Hello World' and nothing else.\"}],\n",
    "            temperature=0,\n",
    "            call_type='test'\n",
    "        )\n",
    "        print(f\"✅ API test successful! Response: {response.strip()}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"❌ API test failed: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Chain Functions\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "\n",
    "# Pydantic models for structured outputs\n",
    "class EvaluationResult(BaseModel):\n",
    "    hallucinations: List[str]\n",
    "    template_needing_improvement: str\n",
    "    quality_notes: str\n",
    "    improvement_suggestions: str\n",
    "\n",
    "\n",
    "def draft_letter(general_template: str, specific_template: str, case: str) -> str:\n",
    "    \"\"\"Generate a letter from templates and case information.\"\"\"\n",
    "    print(\"📝 Drafting letter...\")\n",
    "    \n",
    "    # Combine templates\n",
    "    combined_template = combine_templates(general_template, specific_template)\n",
    "    \n",
    "    prompt = f\"\"\"You are an expert paraplanner writing flawless client letters.\n",
    "\n",
    "Template/Guidance:\n",
    "{combined_template}\n",
    "\n",
    "Case Information:\n",
    "{case}\n",
    "\n",
    "Generate a professional letter following the template structure and incorporating the case details.\n",
    "Output only the letter content in HTML format, no explanations.\"\"\"\n",
    "    \n",
    "    try:\n",
    "        result = provider_llm(\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=TEMPERATURE_DRAFT,\n",
    "            call_type='draft_letter'\n",
    "        )\n",
    "        print(f\"✅ Letter drafted successfully ({len(result)} chars)\")\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to draft letter: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def evaluate_letter(letter: str, source: str, general_template: str, specific_template: str) -> Dict[str, any]:\n",
    "    \"\"\"Evaluate letter for hallucinations and quality with structured output.\"\"\"\n",
    "    print(\"🔍 Evaluating letter...\")\n",
    "    \n",
    "    prompt = f\"\"\"You are a strict auditor evaluating a client letter.\n",
    "\n",
    "Letter to audit:\n",
    "{letter}\n",
    "\n",
    "Source information:\n",
    "{source}\n",
    "\n",
    "General template used:\n",
    "{general_template[:500]}...\n",
    "\n",
    "Specific template used:\n",
    "{specific_template[:500] if specific_template else \"None\"}...\n",
    "\n",
    "Provide a comprehensive evaluation including:\n",
    "1. List any claims in the letter that are NOT supported by the source information (hallucinations)\n",
    "2. Note which template (general or specific) might need improvement (must be one of: general, specific, both, none)\n",
    "3. Overall quality assessment\"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = client.models.generate_content(\n",
    "            model=MODEL,\n",
    "            contents=prompt,\n",
    "            config=types.GenerateContentConfig(\n",
    "                temperature=0,\n",
    "                candidate_count=1,\n",
    "                response_mime_type=\"application/json\",\n",
    "                response_schema=EvaluationResult\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Parse the response\n",
    "        evaluation = response.parsed\n",
    "        \n",
    "        print(f\"✅ Evaluation complete: {len(evaluation.hallucinations)} hallucinations found\")\n",
    "        \n",
    "        # Convert to dict for compatibility\n",
    "        return {\n",
    "            \"hallucinations\": evaluation.hallucinations,\n",
    "            \"template_needing_improvement\": evaluation.template_needing_improvement,\n",
    "            \"quality_notes\": evaluation.quality_notes,\n",
    "            \"improvement_suggestions\": evaluation.improvement_suggestions\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to evaluate letter: {e}\")\n",
    "        return {\n",
    "            \"hallucinations\": [],\n",
    "            \"template_needing_improvement\": \"unknown\",\n",
    "            \"quality_notes\": f\"Evaluation failed: {str(e)}\",\n",
    "            \"improvement_suggestions\": \"\"\n",
    "        }\n",
    "\n",
    "\n",
    "def improve_template_with_ai(template: str, template_type: str, evaluation: Dict, letter: str = None) -> str:\n",
    "    \"\"\"Improve a template based on AI evaluation WITHOUT seeing the actual letter.\"\"\"\n",
    "    print(f\"🔧 AI improving {template_type} template...\")\n",
    "    \n",
    "    hallucinations = evaluation.get(\"hallucinations\", [])\n",
    "    suggestions = evaluation.get(\"improvement_suggestions\", \"\")\n",
    "    \n",
    "    # Create abstract examples of issues WITHOUT showing actual letter content\n",
    "    issue_patterns = []\n",
    "    for h in hallucinations[:5]:\n",
    "        # Extract the type of issue without the specific content\n",
    "        if isinstance(h, dict):\n",
    "            issue_type = h.get('type', 'unknown')\n",
    "            issue_patterns.append(f\"- {issue_type} issue found\")\n",
    "        else:\n",
    "            # Try to categorize the hallucination type\n",
    "            if any(word in str(h).lower() for word in ['date', 'time', 'when']):\n",
    "                issue_patterns.append(\"- Date/time related issue\")\n",
    "            elif any(word in str(h).lower() for word in ['name', 'person', 'company']):\n",
    "                issue_patterns.append(\"- Name/entity related issue\")\n",
    "            elif any(word in str(h).lower() for word in ['amount', 'fee', 'charge', '%']):\n",
    "                issue_patterns.append(\"- Financial figure related issue\")\n",
    "            else:\n",
    "                issue_patterns.append(\"- Unsupported claim issue\")\n",
    "    \n",
    "    prompt = f\"\"\"You are an LLM prompt engineer improving a template.\n",
    "\n",
    "Template Type: {template_type}\n",
    "Current template:\n",
    "{template}\n",
    "\n",
    "Issues found (WITHOUT specific client details):\n",
    "- Total hallucinations: {len(hallucinations)}\n",
    "- Issue patterns:\n",
    "{chr(10).join(issue_patterns)}\n",
    "\n",
    "Improvement suggestions from evaluation:\n",
    "{suggestions}\n",
    "\n",
    "IMPORTANT: You are improving the TEMPLATE ONLY. You have NOT seen any actual client letter or specific details.\n",
    "\n",
    "Improve the template to prevent these types of issues while maintaining its structure and purpose.\n",
    "Make the template more specific about:\n",
    "1. What information to extract from case files\n",
    "2. What NOT to include or make up\n",
    "3. How to handle missing information\n",
    "\n",
    "Output only the improved template.\"\"\"\n",
    "    \n",
    "    try:\n",
    "        result = provider_llm(\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0,\n",
    "            call_type='improve_template'\n",
    "        )\n",
    "        print(f\"✅ {template_type.capitalize()} template improved\")\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to improve {template_type} template: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def improve_template_with_feedback(template: str, template_type: str, feedback: str) -> str:\n",
    "    \"\"\"Improve a template based on manual feedback.\"\"\"\n",
    "    print(f\"🔧 Manually improving {template_type} template...\")\n",
    "    \n",
    "    prompt = f\"\"\"You are an LLM prompt engineer improving a template based on user feedback.\n",
    "\n",
    "Template Type: {template_type}\n",
    "Current template:\n",
    "{template}\n",
    "\n",
    "User feedback:\n",
    "{feedback}\n",
    "\n",
    "Improve the template based on this feedback while maintaining its structure and purpose.\n",
    "Output only the improved template.\"\"\"\n",
    "    \n",
    "    try:\n",
    "        result = provider_llm(\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0,\n",
    "            call_type='improve_template_manual'\n",
    "        )\n",
    "        print(f\"✅ {template_type.capitalize()} template improved with manual feedback\")\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to improve {template_type} template: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def generate_ai_feedback(letter: str, evaluation: Dict, case_content: str) -> str:\n",
    "    \"\"\"Generate AI feedback for template improvement WITHOUT exposing letter content.\"\"\"\n",
    "    hallucinations = evaluation.get(\"hallucinations\", [])\n",
    "    quality_notes = evaluation.get(\"quality_notes\", \"\")\n",
    "    \n",
    "    # Analyze patterns without revealing specific content\n",
    "    patterns = {\n",
    "        'dates': 0,\n",
    "        'names': 0,\n",
    "        'amounts': 0,\n",
    "        'unsupported': 0\n",
    "    }\n",
    "    \n",
    "    for h in hallucinations:\n",
    "        h_str = str(h).lower()\n",
    "        if any(word in h_str for word in ['date', 'time', 'when', 'april', 'may', 'june']):\n",
    "            patterns['dates'] += 1\n",
    "        elif any(word in h_str for word in ['name', 'person', 'company', 'mr', 'mrs']):\n",
    "            patterns['names'] += 1\n",
    "        elif any(word in h_str for word in ['amount', 'fee', 'charge', '%', '£', '$']):\n",
    "            patterns['amounts'] += 1\n",
    "        else:\n",
    "            patterns['unsupported'] += 1\n",
    "    \n",
    "    prompt = f\"\"\"You are a senior paraplanner providing feedback to improve letter templates.\n",
    "\n",
    "Evaluation results:\n",
    "- Total hallucinations found: {len(hallucinations)}\n",
    "- Quality notes: {quality_notes}\n",
    "- Template needing improvement: {evaluation.get('template_needing_improvement', 'unknown')}\n",
    "\n",
    "Pattern analysis (no specific details):\n",
    "- Date-related issues: {patterns['dates']}\n",
    "- Name-related issues: {patterns['names']}\n",
    "- Amount-related issues: {patterns['amounts']}\n",
    "- Unsupported claims: {patterns['unsupported']}\n",
    "\n",
    "Provide specific, actionable feedback to improve the templates. Focus on:\n",
    "1. How to prevent these TYPES of hallucinations\n",
    "2. What specific guidance to add to templates\n",
    "3. Which template needs the most work\n",
    "\n",
    "Be concise and practical. Do NOT reference any specific client details.\"\"\"\n",
    "\n",
    "    return provider_llm(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.3,\n",
    "        call_type='generate_feedback'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3.5: Evaluation Types and Structures\n",
    "\n",
    "from enum import Enum\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "class HallucinationType(Enum):\n",
    "    \"\"\"Types of hallucinations we detect\"\"\"\n",
    "    FACTUAL_ERROR = \"factual_error\"  # Contradicts case file\n",
    "    UNSUPPORTED_CLAIM = \"unsupported_claim\"  # Not found in case\n",
    "    FABRICATED_DETAIL = \"fabricated_detail\"  # Completely made up\n",
    "    INCONSISTENCY = \"inconsistency\"  # Contradicts elsewhere in letter\n",
    "\n",
    "@dataclass\n",
    "class CaseFact:\n",
    "    \"\"\"A fact extracted from case files\"\"\"\n",
    "    content: str  # The actual fact\n",
    "    fact_type: str  # date/name/amount/reference/claim/quote\n",
    "    source_file: str  # Which case file it came from\n",
    "    context: str  # Surrounding text for verification\n",
    "    \n",
    "    def to_dict(self):\n",
    "        return asdict(self)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dict(cls, data):\n",
    "        return cls(**data)\n",
    "\n",
    "@dataclass\n",
    "class LetterFact:\n",
    "    \"\"\"A fact extracted from the letter\"\"\"\n",
    "    content: str  # The actual fact/claim\n",
    "    fact_type: str  # date/name/amount/reference/claim/quote\n",
    "    location: str  # Where in letter (section/paragraph)\n",
    "    requires_verification: bool = True\n",
    "\n",
    "@dataclass\n",
    "class FactVerification:\n",
    "    \"\"\"Result of verifying a letter fact\"\"\"\n",
    "    letter_fact: LetterFact\n",
    "    verdict: str  # SUPPORTED/CONTRADICTED/UNSUPPORTED\n",
    "    explanation: str\n",
    "    supporting_case_facts: List[CaseFact]\n",
    "    correct_info: Optional[str] = None\n",
    "    hallucination_type: Optional[HallucinationType] = None\n",
    "\n",
    "# State for the evaluation workflow\n",
    "class EvaluationState(TypedDict):\n",
    "    letter: str\n",
    "    case_facts: List[CaseFact]  # Pre-extracted from case files\n",
    "    letter_facts: List[LetterFact]\n",
    "    verifications: List[FactVerification]\n",
    "    hallucinations: List[Dict]\n",
    "    score: float\n",
    "    detailed_report: Dict[str, any]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4.5: Case Fact Extraction (One-time per session)\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "\n",
    "# Pydantic models for case facts extraction\n",
    "class CaseFactExtraction(BaseModel):\n",
    "    content: str\n",
    "    fact_type: str\n",
    "    source_file: str\n",
    "    context: str\n",
    "\n",
    "class CaseFactsResponse(BaseModel):\n",
    "    facts: List[CaseFactExtraction]\n",
    "\n",
    "\n",
    "def extract_case_facts_once(case_content: str, case_folder: str) -> List[CaseFact]:\n",
    "    \"\"\"Extract all verifiable facts from case files - called ONCE per session.\n",
    "    \n",
    "    This is expensive so we cache the results for reuse across iterations.\n",
    "    \"\"\"\n",
    "    print(\"📊 Extracting facts from case files (one-time operation)...\")\n",
    "    \n",
    "    prompt = f\"\"\"You are a meticulous fact extractor for legal/financial documents.\n",
    "    \n",
    "Extract ALL verifiable facts from these case files:\n",
    "\n",
    "{case_content}\n",
    "\n",
    "For each fact, extract:\n",
    "1. The exact fact/information\n",
    "2. Type: date, name, amount, reference, claim, quote, or other\n",
    "3. Which file it came from (based on file markers in the text)\n",
    "4. Surrounding context (1-2 sentences around the fact)\n",
    "\n",
    "Focus on:\n",
    "- ALL dates (meetings, deadlines, document dates, birth dates, etc.)\n",
    "- ALL names (people, companies, products, places)\n",
    "- ALL numbers (amounts, percentages, reference numbers, ages)\n",
    "- ALL specific claims, decisions, or agreements\n",
    "- ALL quoted statements\n",
    "- ALL document references\n",
    "\n",
    "Be exhaustive - we'll use this as our source of truth.\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Use provider_llm with structured output\n",
    "        facts_response = provider_llm(\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.1,\n",
    "            response_format=CaseFactsResponse,\n",
    "            call_type='extract_case_facts'\n",
    "        )\n",
    "        \n",
    "        # Parse the response\n",
    "    \n",
    "        # Convert to CaseFact objects\n",
    "        case_facts = []\n",
    "        for fact in facts_response.get('facts', []):\n",
    "            case_facts.append(CaseFact(\n",
    "                content=fact.get('content', ''),\n",
    "                fact_type=fact.get('fact_type', ''),\n",
    "                source_file=fact.get('source_file', ''),\n",
    "                context=fact.get('context', '')\n",
    "            ))\n",
    "        \n",
    "        print(f\"✅ Extracted {len(case_facts)} facts from case files\")\n",
    "\n",
    "        \n",
    "        # Group by type for summary\n",
    "        fact_types = {}\n",
    "        for fact in case_facts:\n",
    "            fact_types[fact.fact_type] = fact_types.get(fact.fact_type, 0) + 1\n",
    "        \n",
    "        print(\"📋 Fact breakdown:\")\n",
    "        for fact_type, count in sorted(fact_types.items()):\n",
    "            print(f\"   - {fact_type}: {count}\")\n",
    "        \n",
    "        return case_facts\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to extract case facts: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def save_case_facts(session_dir: Path, case_facts: List[CaseFact]) -> None:\n",
    "    \"\"\"Save extracted case facts to file for debugging/review.\"\"\"\n",
    "    facts_file = session_dir / \"case_facts.json\"\n",
    "    \n",
    "    facts_data = [fact.to_dict() for fact in case_facts]\n",
    "    \n",
    "    with open(facts_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(facts_data, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"💾 Saved case facts to {facts_file.name}\")\n",
    "\n",
    "\n",
    "def load_case_facts(session_dir: Path) -> List[CaseFact]:\n",
    "    \"\"\"Load previously extracted case facts.\"\"\"\n",
    "    facts_file = session_dir / \"case_facts.json\"\n",
    "    \n",
    "    if not facts_file.exists():\n",
    "        return []\n",
    "    \n",
    "    with open(facts_file, 'r', encoding='utf-8') as f:\n",
    "        facts_data = json.load(f)\n",
    "    \n",
    "    return [CaseFact.from_dict(fact_dict) for fact_dict in facts_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5.5: Enhanced Evaluation Workflow\n",
    "\n",
    "from langgraph.graph import StateGraph, END\n",
    "from pydantic import BaseModel\n",
    "from typing import List, Optional\n",
    "import concurrent.futures\n",
    "import time\n",
    "\n",
    "# Configuration for batch processing\n",
    "VERIFICATION_BATCH_SIZE = 5  # Number of facts to verify in one API call\n",
    "MAX_PARALLEL_BATCHES = 3    # Number of concurrent batch verifications\n",
    "\n",
    "# Pydantic models for structured outputs\n",
    "class LetterFactExtraction(BaseModel):\n",
    "    content: str\n",
    "    fact_type: str\n",
    "    location: str\n",
    "    requires_verification: bool\n",
    "\n",
    "class LetterFactsResponse(BaseModel):\n",
    "    facts: List[LetterFactExtraction]\n",
    "\n",
    "class FactVerificationResult(BaseModel):\n",
    "    verdict: str\n",
    "    explanation: str\n",
    "    correct_info: Optional[str] = None\n",
    "    hallucination_type: Optional[str] = None\n",
    "\n",
    "class BatchVerificationResult(BaseModel):\n",
    "    fact_content: str\n",
    "    verdict: str\n",
    "    explanation: str\n",
    "    correct_info: Optional[str] = None\n",
    "    hallucination_type: Optional[str] = None\n",
    "\n",
    "class BatchVerificationResponse(BaseModel):\n",
    "    verifications: List[BatchVerificationResult]\n",
    "\n",
    "\n",
    "def extract_letter_facts_node(state: EvaluationState) -> EvaluationState:\n",
    "    \"\"\"Extract all facts from the generated letter with structured output.\"\"\"\n",
    "    print(\"  📋 Extracting facts from letter...\")\n",
    "    \n",
    "    prompt = f\"\"\"You are a meticulous fact extractor analyzing a financial advisory letter.\n",
    "\n",
    "Letter to analyze:\n",
    "{state['letter']}\n",
    "\n",
    "Extract the MOST IMPORTANT verifiable facts/claims from the letter (maximum 20-30 facts):\n",
    "1. The exact fact/claim as written\n",
    "2. Type: date, name, amount, reference, claim, quote, or other\n",
    "3. Location in letter (section/paragraph description)\n",
    "4. Whether it requires verification (true for all factual claims)\n",
    "\n",
    "Focus on KEY facts only:\n",
    "- Critical dates (meeting dates, deadlines)\n",
    "- Client and company names\n",
    "- Specific amounts or percentages\n",
    "- Important claims or recommendations\n",
    "- Key document references\n",
    "\n",
    "IMPORTANT: Only extract the most significant facts - aim for 100 facts maximum.\"\"\"\n",
    "    \n",
    "    try:\n",
    "        facts_response = provider_llm(\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0,  \n",
    "            response_format=LetterFactsResponse\n",
    "        )\n",
    "        \n",
    "        \n",
    "        \n",
    "        letter_facts = []\n",
    "        for fact in facts_response.get('facts', []):\n",
    "            letter_facts.append(LetterFact(\n",
    "                content=fact.get('content', ''),\n",
    "                fact_type=fact.get('fact_type', ''),\n",
    "                location=fact.get('location', ''),\n",
    "                requires_verification=fact.get('requires_verification', True)\n",
    "            ))\n",
    "        \n",
    "        \n",
    "            \n",
    "            letter_facts = letter_facts\n",
    "        \n",
    "        state['letter_facts'] = letter_facts\n",
    "        print(f\"  ✅ Extracted {len(letter_facts)} facts from letter\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ❌ Failed to extract letter facts: {e}\")\n",
    "        state['letter_facts'] = []\n",
    "    \n",
    "    return state\n",
    "\n",
    "\n",
    "def verify_fact_batch(batch: List[LetterFact], case_facts: List[CaseFact], batch_num: int) -> List[FactVerification]:\n",
    "    \"\"\"Verify a batch of facts in a single API call.\"\"\"\n",
    "    print(f\"  🔍 Verifying batch {batch_num} ({len(batch)} facts)...\")\n",
    "    \n",
    "    # Build a comprehensive prompt for batch verification\n",
    "    facts_to_verify = []\n",
    "    for fact in batch:\n",
    "        facts_to_verify.append(f\"\"\"\n",
    "Fact {batch.index(fact) + 1}:\n",
    "- Content: {fact.content}\n",
    "- Type: {fact.fact_type}\n",
    "- Location: {fact.location}\"\"\")\n",
    "    \n",
    "    # Get relevant case facts for all facts in batch\n",
    "    all_keywords = []\n",
    "    for fact in batch:\n",
    "        all_keywords.extend(fact.content.lower().split()[:3])\n",
    "    \n",
    "    case_facts_context = \"\\n\".join([\n",
    "        f\"- {cf.content} (Type: {cf.fact_type}, From: {cf.source_file})\"\n",
    "        for cf in case_facts\n",
    "        if any(keyword in cf.content.lower() for keyword in all_keywords)\n",
    "    ][:30])  # Limit context size\n",
    "    \n",
    "    prompt = f\"\"\"You are a fact-checker verifying multiple claims in a letter against source documents.\n",
    "\n",
    "FACTS TO VERIFY:\n",
    "{chr(10).join(facts_to_verify)}\n",
    "\n",
    "RELEVANT CASE FACTS:\n",
    "{case_facts_context if case_facts_context else \"No directly matching facts found\"}\n",
    "\n",
    "CASE FACTS SUMMARY:\n",
    "Total facts available: {len(case_facts)}\n",
    "Types: {', '.join(set(cf.fact_type for cf in case_facts))}\n",
    "\n",
    "For EACH fact above:\n",
    "1. Determine if it is SUPPORTED, CONTRADICTED, or UNSUPPORTED by case facts\n",
    "2. If CONTRADICTED, provide the correct information\n",
    "3. Categorize any issues with lowercase values: factual_error, unsupported_claim, fabricated_detail, or null\n",
    "\n",
    "Return a verification for each fact in order.\"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = provider_llm(\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0,\n",
    "            response_format=BatchVerificationResponse\n",
    "        )\n",
    "        \n",
    "        verifications = []\n",
    "        results = response.get('verifications', [])\n",
    "        \n",
    "        for i, (fact, result) in enumerate(zip(batch, results)):\n",
    "            # Handle enum case mismatch\n",
    "            hallucination_type_str = result.get('hallucination_type')\n",
    "            hallucination_type = None\n",
    "            if hallucination_type_str and result.get('verdict') != 'SUPPORTED':\n",
    "                type_mapping = {\n",
    "                    'factual_error': HallucinationType.FACTUAL_ERROR,\n",
    "                    'unsupported_claim': HallucinationType.UNSUPPORTED_CLAIM,\n",
    "                    'fabricated_detail': HallucinationType.FABRICATED_DETAIL,\n",
    "                    'inconsistency': HallucinationType.INCONSISTENCY\n",
    "                }\n",
    "                hallucination_type = type_mapping.get(hallucination_type_str.lower())\n",
    "            \n",
    "            # Find supporting case facts if supported\n",
    "            supporting_facts = []\n",
    "            if result.get('verdict') == 'SUPPORTED':\n",
    "                for cf in case_facts:\n",
    "                    if any(word in cf.content.lower() for word in fact.content.lower().split()):\n",
    "                        supporting_facts.append(cf)\n",
    "                        if len(supporting_facts) >= 3:\n",
    "                            break\n",
    "            \n",
    "            verification = FactVerification(\n",
    "                letter_fact=fact,\n",
    "                verdict=result.get('verdict', 'UNSUPPORTED'),\n",
    "                explanation=result.get('explanation', 'No explanation provided'),\n",
    "                supporting_case_facts=supporting_facts,\n",
    "                correct_info=result.get('correct_info'),\n",
    "                hallucination_type=hallucination_type\n",
    "            )\n",
    "            verifications.append(verification)\n",
    "        \n",
    "        print(f\"  ✅ Batch {batch_num} verified successfully\")\n",
    "        return verifications\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ❌ Failed to verify batch {batch_num}: {str(e)}\")\n",
    "        # Return unsupported for all facts in failed batch\n",
    "        return [\n",
    "            FactVerification(\n",
    "                letter_fact=fact,\n",
    "                verdict='UNSUPPORTED',\n",
    "                explanation=f\"Batch verification failed: {str(e)}\",\n",
    "                supporting_case_facts=[],\n",
    "                hallucination_type=HallucinationType.UNSUPPORTED_CLAIM\n",
    "            ) for fact in batch\n",
    "        ]\n",
    "\n",
    "\n",
    "def verify_facts_node(state: EvaluationState) -> EvaluationState:\n",
    "    \"\"\"Verify facts in parallel batches.\"\"\"\n",
    "    print(\"  🔍 Verifying facts in parallel batches...\")\n",
    "    \n",
    "    facts_to_verify = state['letter_facts'][:20]  # Limit total facts\n",
    "    total_facts = len(facts_to_verify)\n",
    "    \n",
    "    # Split facts into batches\n",
    "    batches = []\n",
    "    for i in range(0, total_facts, VERIFICATION_BATCH_SIZE):\n",
    "        batch = facts_to_verify[i:i + VERIFICATION_BATCH_SIZE]\n",
    "        batches.append(batch)\n",
    "    \n",
    "    print(f\"  📦 Created {len(batches)} batches (size {VERIFICATION_BATCH_SIZE} each)\")\n",
    "    \n",
    "    all_verifications = []\n",
    "    \n",
    "    # Process batches in parallel\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_PARALLEL_BATCHES) as executor:\n",
    "        # Submit all batch verification tasks\n",
    "        future_to_batch = {\n",
    "            executor.submit(verify_fact_batch, batch, state['case_facts'], i+1): (i, batch)\n",
    "            for i, batch in enumerate(batches)\n",
    "        }\n",
    "        \n",
    "        # Collect results as they complete\n",
    "        for future in concurrent.futures.as_completed(future_to_batch):\n",
    "            batch_idx, batch = future_to_batch[future]\n",
    "            try:\n",
    "                verifications = future.result()\n",
    "                all_verifications.extend(verifications)\n",
    "                \n",
    "                # Quick summary of batch results\n",
    "                supported = sum(1 for v in verifications if v.verdict == 'SUPPORTED')\n",
    "                print(f\"     Batch {batch_idx+1}: {supported}/{len(verifications)} supported\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  ❌ Batch {batch_idx+1} failed: {str(e)}\")\n",
    "                # Add failed verifications for this batch\n",
    "                for fact in batch:\n",
    "                    all_verifications.append(FactVerification(\n",
    "                        letter_fact=fact,\n",
    "                        verdict='UNSUPPORTED',\n",
    "                        explanation=f\"Verification failed: {str(e)}\",\n",
    "                        supporting_case_facts=[],\n",
    "                        hallucination_type=HallucinationType.UNSUPPORTED_CLAIM\n",
    "                    ))\n",
    "    \n",
    "    # Sort verifications back to original order\n",
    "    state['verifications'] = sorted(all_verifications, \n",
    "                                  key=lambda v: facts_to_verify.index(v.letter_fact))\n",
    "    \n",
    "    print(f\"  ✅ Completed parallel verification of {len(state['verifications'])} facts\")\n",
    "    \n",
    "    return state\n",
    "\n",
    "\n",
    "def compile_results_node(state: EvaluationState) -> EvaluationState:\n",
    "    \"\"\"Compile verification results into final evaluation with metrics.\"\"\"\n",
    "    print(\"  📊 Compiling evaluation results...\")\n",
    "    \n",
    "    # Count verdicts\n",
    "    verdict_counts = {\n",
    "        'SUPPORTED': 0,\n",
    "        'CONTRADICTED': 0,\n",
    "        'UNSUPPORTED': 0\n",
    "    }\n",
    "    \n",
    "    hallucinations = []\n",
    "    hallucination_types = {}\n",
    "    \n",
    "    for verification in state['verifications']:\n",
    "        verdict_counts[verification.verdict] += 1\n",
    "        \n",
    "        if verification.verdict in ['CONTRADICTED', 'UNSUPPORTED']:\n",
    "            # This is a hallucination\n",
    "            hallucination = {\n",
    "                'fact': verification.letter_fact.content,\n",
    "                'type': verification.hallucination_type.value if verification.hallucination_type else 'unknown',\n",
    "                'location': verification.letter_fact.location,\n",
    "                'explanation': verification.explanation,\n",
    "                'verdict': verification.verdict\n",
    "            }\n",
    "            \n",
    "            if verification.correct_info:\n",
    "                hallucination['correct_info'] = verification.correct_info\n",
    "            \n",
    "            hallucinations.append(hallucination)\n",
    "            \n",
    "            # Track hallucination types\n",
    "            h_type = verification.hallucination_type.value if verification.hallucination_type else 'unknown'\n",
    "            hallucination_types[h_type] = hallucination_types.get(h_type, 0) + 1\n",
    "    \n",
    "    # Calculate score (percentage of supported facts)\n",
    "    total_facts = len(state['verifications'])\n",
    "    score = (verdict_counts['SUPPORTED'] / total_facts * 100) if total_facts > 0 else 100\n",
    "    \n",
    "    # Determine which template needs work\n",
    "    if len(hallucinations) == 0:\n",
    "        template_needing_improvement = 'none'\n",
    "    elif hallucination_types.get('FABRICATED_DETAIL', 0) > hallucination_types.get('FACTUAL_ERROR', 0):\n",
    "        template_needing_improvement = 'specific'  # Specific details are being fabricated\n",
    "    elif any('date' in h['fact'].lower() or 'amount' in h['fact'].lower() for h in hallucinations):\n",
    "        template_needing_improvement = 'general'  # General structure issues\n",
    "    else:\n",
    "        template_needing_improvement = 'both'\n",
    "    \n",
    "    # Build evaluation metrics snapshot\n",
    "    metrics_snapshot = {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'total_facts_extracted': len(state['letter_facts']),\n",
    "        'total_facts_verified': total_facts,\n",
    "        'verification_summary': verdict_counts,\n",
    "        'hallucination_breakdown': hallucination_types,\n",
    "        'accuracy_score': round(score, 1),\n",
    "        'verdict': 'PASS' if score >= 95 else 'NEEDS_IMPROVEMENT',\n",
    "        'template_recommendation': template_needing_improvement,\n",
    "        'processing_stats': {\n",
    "            'batch_size': VERIFICATION_BATCH_SIZE,\n",
    "            'parallel_batches': MAX_PARALLEL_BATCHES,\n",
    "            'facts_per_batch': VERIFICATION_BATCH_SIZE\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Build detailed report\n",
    "    detailed_report = {\n",
    "        'total_facts_checked': total_facts,\n",
    "        'verification_summary': verdict_counts,\n",
    "        'hallucination_types': hallucination_types,\n",
    "        'score': round(score, 1),\n",
    "        'verdict': 'PASS' if score >= 95 else 'NEEDS_IMPROVEMENT',\n",
    "        'top_issues': hallucinations[:5],  # Top 5 issues\n",
    "        'metrics_snapshot': metrics_snapshot\n",
    "    }\n",
    "    \n",
    "    # Update state\n",
    "    state['hallucinations'] = hallucinations\n",
    "    state['score'] = score\n",
    "    state['detailed_report'] = detailed_report\n",
    "    state['metrics_snapshot'] = metrics_snapshot  # Store for iteration tracking\n",
    "    \n",
    "    # Also return in format compatible with main workflow\n",
    "    evaluation_result = {\n",
    "        'hallucinations': hallucinations,\n",
    "        'template_needing_improvement': template_needing_improvement,\n",
    "        'quality_notes': f\"Score: {score:.1f}% - {len(hallucinations)} issues found\",\n",
    "        'improvement_suggestions': f\"Focus on {template_needing_improvement} template to reduce {', '.join(hallucination_types.keys())}\",\n",
    "        'facts_checked': total_facts,\n",
    "        'score': score,\n",
    "        'detailed_report': detailed_report\n",
    "    }\n",
    "    \n",
    "    print(f\"  ✅ Evaluation complete: {score:.1f}% accuracy, {len(hallucinations)} issues\")\n",
    "    \n",
    "    return state\n",
    "\n",
    "\n",
    "def build_evaluation_workflow():\n",
    "    \"\"\"Build the enhanced evaluation workflow.\"\"\"\n",
    "    workflow = StateGraph(EvaluationState)\n",
    "    \n",
    "    # Add nodes\n",
    "    workflow.add_node(\"extract_letter_facts\", extract_letter_facts_node)\n",
    "    workflow.add_node(\"verify_facts\", verify_facts_node)\n",
    "    workflow.add_node(\"compile_results\", compile_results_node)\n",
    "    \n",
    "    # Define flow\n",
    "    workflow.set_entry_point(\"extract_letter_facts\")\n",
    "    workflow.add_edge(\"extract_letter_facts\", \"verify_facts\")\n",
    "    workflow.add_edge(\"verify_facts\", \"compile_results\")\n",
    "    workflow.add_edge(\"compile_results\", END)\n",
    "    \n",
    "    return workflow.compile()\n",
    "\n",
    "\n",
    "def run_enhanced_evaluation(letter: str, case_facts: List[CaseFact]) -> Dict[str, any]:\n",
    "    \"\"\"Run the enhanced evaluation workflow.\"\"\"\n",
    "    print(\"🔬 Running enhanced evaluation workflow...\")\n",
    "    \n",
    "    # Initialize evaluation state\n",
    "    eval_state = {\n",
    "        'letter': letter,\n",
    "        'case_facts': case_facts,\n",
    "        'letter_facts': [],\n",
    "        'verifications': [],\n",
    "        'hallucinations': [],\n",
    "        'score': 0.0,\n",
    "        'detailed_report': {},\n",
    "        'metrics_snapshot': {}\n",
    "    }\n",
    "    \n",
    "    # Build and run evaluation workflow\n",
    "    eval_app = build_evaluation_workflow()\n",
    "    final_eval_state = eval_app.invoke(eval_state)\n",
    "    \n",
    "    # Extract the evaluation result in format expected by main workflow\n",
    "    return {\n",
    "        'hallucinations': final_eval_state['hallucinations'],\n",
    "        'template_needing_improvement': determine_template_from_results(final_eval_state),\n",
    "        'quality_notes': f\"Score: {final_eval_state['score']:.1f}% - {len(final_eval_state['hallucinations'])} issues found\",\n",
    "        'improvement_suggestions': generate_improvement_suggestions(final_eval_state),\n",
    "        'facts_checked': len(final_eval_state['letter_facts']),\n",
    "        'score': final_eval_state['score'],\n",
    "        'detailed_report': final_eval_state['detailed_report'],\n",
    "        'metrics_snapshot': final_eval_state.get('metrics_snapshot', {})\n",
    "    }\n",
    "\n",
    "\n",
    "def determine_template_from_results(eval_state: Dict) -> str:\n",
    "    \"\"\"Determine which template needs improvement based on evaluation.\"\"\"\n",
    "    hallucinations = eval_state['hallucinations']\n",
    "    \n",
    "    if not hallucinations:\n",
    "        return 'none'\n",
    "    \n",
    "    # Analyze hallucination patterns\n",
    "    fabricated = sum(1 for h in hallucinations if h.get('type') == 'FABRICATED_DETAIL')\n",
    "    factual_errors = sum(1 for h in hallucinations if h.get('type') == 'FACTUAL_ERROR')\n",
    "    unsupported = sum(1 for h in hallucinations if h.get('type') == 'UNSUPPORTED_CLAIM')\n",
    "    \n",
    "    if fabricated > factual_errors:\n",
    "        return 'specific'  # Too many made-up details\n",
    "    elif factual_errors > unsupported:\n",
    "        return 'general'  # Structure causing errors\n",
    "    else:\n",
    "        return 'both'  # Both need work\n",
    "\n",
    "\n",
    "def generate_improvement_suggestions(eval_state: Dict) -> str:\n",
    "    \"\"\"Generate specific improvement suggestions based on evaluation.\"\"\"\n",
    "    report = eval_state['detailed_report']\n",
    "    hallucination_types = report.get('hallucination_types', {})\n",
    "    \n",
    "    suggestions = []\n",
    "    \n",
    "    if hallucination_types.get('FABRICATED_DETAIL', 0) > 0:\n",
    "        suggestions.append(\"Add more specific constraints to prevent fabrication\")\n",
    "    \n",
    "    if hallucination_types.get('FACTUAL_ERROR', 0) > 0:\n",
    "        suggestions.append(\"Clarify fact extraction rules in templates\")\n",
    "    \n",
    "    if hallucination_types.get('UNSUPPORTED_CLAIM', 0) > 0:\n",
    "        suggestions.append(\"Emphasize using only provided information\")\n",
    "    \n",
    "    return \"; \".join(suggestions) if suggestions else \"Templates performing well\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: LangGraph Workflow\n",
    "\n",
    "# Define state - Updated to include case facts\n",
    "class LetterState(TypedDict):\n",
    "    # Input data\n",
    "    case_folder: str\n",
    "    template_type: str\n",
    "    client_name: str\n",
    "    session_dir: Path\n",
    "    \n",
    "    # Current templates (updated each iteration)\n",
    "    general_template: str\n",
    "    specific_template: str\n",
    "    \n",
    "    # Case content and facts\n",
    "    case_content: str\n",
    "    case_facts: List[CaseFact]  # Extracted once, reused\n",
    "    case_facts_extracted: bool\n",
    "    \n",
    "    # Current iteration\n",
    "    iteration: int\n",
    "    letter: str\n",
    "    evaluation: Dict[str, any]\n",
    "    \n",
    "    # User interaction\n",
    "    user_decision: str  # 'accept', 'ai_feedback', 'manual_feedback'\n",
    "    user_feedback: Optional[str]\n",
    "    \n",
    "    # Track what was updated\n",
    "    general_template_updated: bool\n",
    "    specific_template_updated: bool\n",
    "    \n",
    "    # History tracking\n",
    "    iterations_data: List[Dict]\n",
    "\n",
    "\n",
    "# Define nodes with debugging\n",
    "def draft_node(state: LetterState) -> LetterState:\n",
    "    \"\"\"Draft the letter.\"\"\"\n",
    "    print(f\"\\n🔄 DRAFT NODE: Starting iteration {state['iteration'] + 1}...\")\n",
    "    \n",
    "    # Reset LLM metrics for this iteration\n",
    "    reset_llm_metrics()\n",
    "    \n",
    "    try:\n",
    "        state['iteration'] += 1\n",
    "        state['letter'] = draft_letter(\n",
    "            state['general_template'], \n",
    "            state['specific_template'], \n",
    "            state['case_content']\n",
    "        )\n",
    "        print(\"🔄 DRAFT NODE: Completed successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"🔄 DRAFT NODE: Failed - {e}\")\n",
    "        raise\n",
    "    return state\n",
    "\n",
    "\n",
    "def evaluate_node(state: LetterState) -> LetterState:\n",
    "    \"\"\"Evaluate the letter using enhanced evaluation.\"\"\"\n",
    "    print(\"\\n🔄 EVALUATE NODE: Starting...\")\n",
    "    \n",
    "    # Debug: Check if case facts are available\n",
    "    print(f\"   Case facts available: {len(state.get('case_facts', [])) if state.get('case_facts') else 0}\")\n",
    "    \n",
    "    try:\n",
    "        # Use enhanced evaluation with cached case facts\n",
    "        if state.get('case_facts'):\n",
    "            # Use the enhanced evaluation\n",
    "            print(\"   Using enhanced evaluation workflow...\")\n",
    "            state['evaluation'] = run_enhanced_evaluation(\n",
    "                state['letter'], \n",
    "                state['case_facts']\n",
    "            )\n",
    "        else:\n",
    "            # Fallback to original evaluation\n",
    "            print(\"⚠️  No case facts available, using simple evaluation\")\n",
    "            state['evaluation'] = evaluate_letter(\n",
    "                state['letter'], \n",
    "                state['case_content'],\n",
    "                state['general_template'],\n",
    "                state['specific_template']\n",
    "            )\n",
    "        \n",
    "        state['hallucinations'] = state['evaluation'].get('hallucinations', [])\n",
    "        state['evaluation_notes'] = state['evaluation'].get('quality_notes', '')\n",
    "        print(\"🔄 EVALUATE NODE: Completed successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"🔄 EVALUATE NODE: Failed - {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        state['evaluation'] = {\n",
    "            \"hallucinations\": [],\n",
    "            \"template_needing_improvement\": \"unknown\",\n",
    "            \"quality_notes\": f\"Evaluation failed: {str(e)}\"\n",
    "        }\n",
    "        state['hallucinations'] = []\n",
    "    return state\n",
    "\n",
    "\n",
    "def human_review_node(state: LetterState) -> LetterState:\n",
    "    \"\"\"Get human input on the evaluation.\"\"\"\n",
    "    print(\"\\n🔄 HUMAN REVIEW NODE: Starting...\")\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ITERATION {state['iteration']} RESULTS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    evaluation = state['evaluation']\n",
    "    hallucinations = evaluation.get('hallucinations', [])\n",
    "    \n",
    "    print(f\"\\n📊 Evaluation Summary:\")\n",
    "    print(f\"  - Hallucinations found: {len(hallucinations)}\")\n",
    "    print(f\"  - Facts checked: {evaluation.get('facts_checked', 'N/A')}\")\n",
    "    print(f\"  - Evaluation score: {evaluation.get('score', 'N/A')}\")\n",
    "    print(f\"  - Template needing work: {evaluation.get('template_needing_improvement', 'unknown')}\")\n",
    "    print(f\"  - Quality notes: {evaluation.get('quality_notes', 'N/A')}\")\n",
    "    \n",
    "    # Show detailed report if available\n",
    "    if 'detailed_report' in evaluation:\n",
    "        report = evaluation['detailed_report']\n",
    "        print(f\"\\n📈 Detailed Analysis:\")\n",
    "        print(f\"  - Verdict: {report.get('verdict', 'N/A')}\")\n",
    "        verification = report.get('verification_summary', {})\n",
    "        print(f\"  - Facts supported: {verification.get('SUPPORTED', 0)}\")\n",
    "        print(f\"  - Facts contradicted: {verification.get('CONTRADICTED', 0)}\")\n",
    "        print(f\"  - Facts unsupported: {verification.get('UNSUPPORTED', 0)}\")\n",
    "    \n",
    "    if hallucinations:\n",
    "        print(f\"\\n🚨 Issues detected:\")\n",
    "        for i, h in enumerate(hallucinations[:5], 1):\n",
    "            # Handle both string and dict hallucinations\n",
    "            if isinstance(h, str):\n",
    "                print(f\"  {i}. {h}\")\n",
    "            else:\n",
    "                print(f\"  {i}. {h.get('fact', h)}  [{h.get('type', 'unknown')}]\")\n",
    "                if 'explanation' in h:\n",
    "                    print(f\"     → {h['explanation']}\")\n",
    "                if 'correct_info' in h and h['correct_info'] != 'Not found in case files':\n",
    "                    print(f\"     ✓ Should be: {h['correct_info']}\")\n",
    "        if len(hallucinations) > 5:\n",
    "            print(f\"  ... and {len(hallucinations) - 5} more\")\n",
    "    \n",
    "    # Save current iteration data\n",
    "    save_iteration_data(state['session_dir'], state['iteration'], state)\n",
    "    \n",
    "    # Print letter location\n",
    "    print(f\"\\n📄 Current letter saved to: {state['session_dir'] / f'letter_iteration_{state['iteration']}.html'}\")\n",
    "    \n",
    "    # Get user decision - Simplified UI\n",
    "    print(\"\\n📋 Options:\")\n",
    "    print(\"1. Accept letter as is\")\n",
    "    print(\"2. Use AI to improve templates based on evaluation\")\n",
    "    print(\"3. Provide manual feedback for template improvement\")\n",
    "    \n",
    "    while True:\n",
    "        choice = input(\"\\nYour choice (1-3): \").strip()\n",
    "        \n",
    "        if choice == \"1\":\n",
    "            state['user_decision'] = 'accept'\n",
    "            print(\"✅ Letter accepted!\")\n",
    "            break\n",
    "        elif choice == \"2\":\n",
    "            state['user_decision'] = 'ai_feedback'\n",
    "            print(\"🤖 Using AI feedback to improve templates...\")\n",
    "            break\n",
    "        elif choice == \"3\":\n",
    "            feedback = input(\"\\nEnter your feedback for template improvement: \").strip()\n",
    "            if feedback:\n",
    "                state['user_feedback'] = feedback\n",
    "                state['user_decision'] = 'manual_feedback'\n",
    "                print(\"📝 Manual feedback recorded\")\n",
    "                break\n",
    "            else:\n",
    "                print(\"❌ No feedback provided, please try again\")\n",
    "        else:\n",
    "            print(\"❌ Invalid choice, please enter 1, 2, or 3\")\n",
    "    \n",
    "    # Add to history\n",
    "    iteration_data = {\n",
    "        'iteration': state['iteration'],\n",
    "        'hallucinations_count': len(hallucinations),\n",
    "        'user_decision': state['user_decision'],\n",
    "        'user_feedback': state.get('user_feedback', ''),\n",
    "        'evaluation': evaluation\n",
    "    }\n",
    "    state['iterations_data'].append(iteration_data)\n",
    "    \n",
    "    print(\"🔄 HUMAN REVIEW NODE: Completed\")\n",
    "    return state\n",
    "\n",
    "\n",
    "def ai_improve_templates_node(state: LetterState) -> LetterState:\n",
    "    \"\"\"Improve templates using AI feedback.\"\"\"\n",
    "    print(\"\\n🔄 AI IMPROVE TEMPLATES NODE: Starting...\")\n",
    "    \n",
    "    evaluation = state['evaluation']\n",
    "    which_template = evaluation.get('template_needing_improvement', 'both')\n",
    "    \n",
    "    # Reset update flags\n",
    "    state['general_template_updated'] = False\n",
    "    state['specific_template_updated'] = False\n",
    "    \n",
    "    try:\n",
    "        # Generate AI feedback\n",
    "        ai_feedback = generate_ai_feedback(\n",
    "            state['letter'], \n",
    "            evaluation, \n",
    "            state['case_content']\n",
    "        )\n",
    "        print(f\"🤖 AI feedback generated\")\n",
    "        \n",
    "        # Improve templates based on evaluation\n",
    "        if which_template in ['general', 'both']:\n",
    "            print(\"📝 Improving general template...\")\n",
    "            state['general_template'] = improve_template_with_ai(\n",
    "                state['general_template'],\n",
    "                'general',\n",
    "                evaluation,\n",
    "                state['letter']\n",
    "            )\n",
    "            state['general_template_updated'] = True\n",
    "        \n",
    "        if which_template in ['specific', 'both']:\n",
    "            print(\"📝 Improving specific template...\")\n",
    "            state['specific_template'] = improve_template_with_ai(\n",
    "                state['specific_template'],\n",
    "                'specific',\n",
    "                evaluation,\n",
    "                state['letter']\n",
    "            )\n",
    "            state['specific_template_updated'] = True\n",
    "        \n",
    "        print(\"🔄 AI IMPROVE TEMPLATES NODE: Completed\")\n",
    "    except Exception as e:\n",
    "        print(f\"🔄 AI IMPROVE TEMPLATES NODE: Failed - {e}\")\n",
    "        raise\n",
    "    \n",
    "    return state\n",
    "\n",
    "\n",
    "def manual_improve_templates_node(state: LetterState) -> LetterState:\n",
    "    \"\"\"Improve templates using manual feedback.\"\"\"\n",
    "    print(\"\\n🔄 MANUAL IMPROVE TEMPLATES NODE: Starting...\")\n",
    "    \n",
    "    feedback = state.get('user_feedback', '')\n",
    "    if not feedback:\n",
    "        print(\"⚠️  No feedback provided, skipping improvement\")\n",
    "        return state\n",
    "    \n",
    "    # Reset update flags\n",
    "    state['general_template_updated'] = False\n",
    "    state['specific_template_updated'] = False\n",
    "    \n",
    "    try:\n",
    "        # Show options BEFORE asking for choice\n",
    "        print(\"\\nWhich template should be improved?\")\n",
    "        print(\"1. General template only\")\n",
    "        print(\"2. Specific template only\")\n",
    "        print(\"3. Both templates\")\n",
    "        \n",
    "        choice = input(\"\\nYour choice (1-3): \").strip()\n",
    "        \n",
    "        if choice in [\"1\", \"3\"]:\n",
    "            print(\"📝 Improving general template with manual feedback...\")\n",
    "            state['general_template'] = improve_template_with_feedback(\n",
    "                state['general_template'],\n",
    "                'general',\n",
    "                feedback\n",
    "            )\n",
    "            state['general_template_updated'] = True\n",
    "        \n",
    "        if choice in [\"2\", \"3\"]:\n",
    "            print(\"📝 Improving specific template with manual feedback...\")\n",
    "            state['specific_template'] = improve_template_with_feedback(\n",
    "                state['specific_template'],\n",
    "                'specific',\n",
    "                feedback\n",
    "            )\n",
    "            state['specific_template_updated'] = True\n",
    "        \n",
    "        if choice not in [\"1\", \"2\", \"3\"]:\n",
    "            print(\"⚠️  Invalid choice, skipping template improvement\")\n",
    "        \n",
    "        print(\"🔄 MANUAL IMPROVE TEMPLATES NODE: Completed\")\n",
    "    except Exception as e:\n",
    "        print(f\"🔄 MANUAL IMPROVE TEMPLATES NODE: Failed - {e}\")\n",
    "        raise\n",
    "    \n",
    "    return state\n",
    "\n",
    "\n",
    "def save_final_node(state: LetterState) -> LetterState:\n",
    "    \"\"\"Save the final accepted letter.\"\"\"\n",
    "    print(\"\\n🔄 SAVE FINAL NODE: Starting...\")\n",
    "    \n",
    "    # Save final letter\n",
    "    final_letter_path = state['session_dir'] / \"final_accepted_letter.html\"\n",
    "    with open(final_letter_path, \"w\") as f:\n",
    "        f.write(state['letter'])\n",
    "    \n",
    "    # Save session summary\n",
    "    save_session_summary(state['session_dir'], state)\n",
    "    \n",
    "    print(f\"✅ Final letter saved to: {final_letter_path}\")\n",
    "    print(\"🔄 SAVE FINAL NODE: Completed\")\n",
    "    \n",
    "    return state\n",
    "\n",
    "\n",
    "def route_human_decision(state: LetterState) -> str:\n",
    "    \"\"\"Route based on human decision.\"\"\"\n",
    "    decision = state.get('user_decision', '')\n",
    "    if decision == 'accept':\n",
    "        return 'save_final'\n",
    "    elif decision == 'ai_feedback':\n",
    "        return 'ai_improve_templates'\n",
    "    elif decision == 'manual_feedback':\n",
    "        return 'manual_improve_templates'\n",
    "    else:\n",
    "        # Default to accept if something goes wrong\n",
    "        return 'save_final'\n",
    "\n",
    "\n",
    "# Build graph\n",
    "def build_letter_graph():\n",
    "    workflow = StateGraph(LetterState)\n",
    "    \n",
    "    # Add nodes\n",
    "    workflow.add_node(\"draft\", draft_node)\n",
    "    workflow.add_node(\"evaluate\", evaluate_node)\n",
    "    workflow.add_node(\"human_review\", human_review_node)\n",
    "    workflow.add_node(\"ai_improve_templates\", ai_improve_templates_node)\n",
    "    workflow.add_node(\"manual_improve_templates\", manual_improve_templates_node)\n",
    "    workflow.add_node(\"save_final\", save_final_node)\n",
    "    \n",
    "    # Add edges\n",
    "    workflow.set_entry_point(\"draft\")\n",
    "    workflow.add_edge(\"draft\", \"evaluate\")\n",
    "    workflow.add_edge(\"evaluate\", \"human_review\")\n",
    "    \n",
    "    # Conditional routing from human review\n",
    "    workflow.add_conditional_edges(\n",
    "        \"human_review\",\n",
    "        route_human_decision,\n",
    "        {\n",
    "            \"save_final\": \"save_final\",\n",
    "            \"ai_improve_templates\": \"ai_improve_templates\",\n",
    "            \"manual_improve_templates\": \"manual_improve_templates\"\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Both improvement nodes loop back to draft\n",
    "    workflow.add_edge(\"ai_improve_templates\", \"draft\")\n",
    "    workflow.add_edge(\"manual_improve_templates\", \"draft\")\n",
    "    \n",
    "    # Save final goes to END\n",
    "    workflow.add_edge(\"save_final\", END)\n",
    "    \n",
    "    return workflow.compile()\n",
    "\n",
    "\n",
    "# Convenience function for running a session\n",
    "def run_interactive_session(case_folder: str, template_type: str = \"annual_review\", max_iterations: int = 10) -> Dict[str, any]:\n",
    "    \"\"\"Run an interactive letter generation session with looping.\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"🚀 STARTING LETTER GENERATION SESSION\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Extract client name\n",
    "    client_name = case_folder.split('/')[-1]\n",
    "    print(f\"📁 Client: {client_name}\")\n",
    "    print(f\"📄 Template type: {template_type}\")\n",
    "    \n",
    "    try:\n",
    "        # Create session folder\n",
    "        session_dir = create_session_folder(client_name)\n",
    "        print(f\"📂 Session folder: {session_dir}\")\n",
    "        \n",
    "        # Read initial data\n",
    "        print(\"\\n📖 Loading data...\")\n",
    "        case_content = read_case_folder(case_folder)\n",
    "        general_template = read_general_template()\n",
    "        specific_template = read_specific_template(template_type)\n",
    "        \n",
    "        print(f\"✅ Loaded {len(case_content)} chars of case data\")\n",
    "        print(f\"✅ Loaded general template ({len(general_template)} chars)\")\n",
    "        print(f\"✅ Loaded specific template ({len(specific_template)} chars)\")\n",
    "        \n",
    "        # Extract case facts ONCE\n",
    "        case_facts = extract_case_facts_once(case_content, case_folder)\n",
    "        \n",
    "        # Save case facts for debugging/review\n",
    "        if case_facts:\n",
    "            save_case_facts(session_dir, case_facts)\n",
    "        else:\n",
    "            print(\"⚠️  WARNING: No case facts extracted! Enhanced evaluation will not be available.\")\n",
    "        \n",
    "        # Initialize state\n",
    "        initial_state = {\n",
    "            \"case_folder\": case_folder,\n",
    "            \"template_type\": template_type,\n",
    "            \"client_name\": client_name,\n",
    "            \"session_dir\": session_dir,\n",
    "            \"case_content\": case_content,\n",
    "            \"case_facts\": case_facts,\n",
    "            \"case_facts_extracted\": True,\n",
    "            \"general_template\": general_template,\n",
    "            \"specific_template\": specific_template,\n",
    "            \"iteration\": 0,\n",
    "            \"letter\": \"\",\n",
    "            \"evaluation\": {},\n",
    "            \"user_decision\": \"\",\n",
    "            \"user_feedback\": None,\n",
    "            \"general_template_updated\": False,\n",
    "            \"specific_template_updated\": False,\n",
    "            \"iterations_data\": []\n",
    "        }\n",
    "        \n",
    "        # Build and run workflow\n",
    "        print(\"\\n🔄 Starting workflow...\")\n",
    "        app = build_letter_graph()\n",
    "        \n",
    "        # Run with iteration limit check\n",
    "        final_state = app.invoke(initial_state)\n",
    "        \n",
    "        # Check if we hit iteration limit\n",
    "        if final_state['iteration'] >= max_iterations:\n",
    "            print(f\"\\n⚠️  Maximum iterations ({max_iterations}) reached!\")\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"✅ SESSION COMPLETED\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"📈 Total iterations: {final_state['iteration']}\")\n",
    "        print(f\"📂 All files saved in: {session_dir}\")\n",
    "        \n",
    "        return final_state\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ SESSION FAILED: {type(e).__name__}: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Display Functions (kept for compatibility)\n",
    "\n",
    "# Note: Most display functionality is now integrated into the workflow itself.\n",
    "# These functions are kept for backward compatibility and standalone use.\n",
    "\n",
    "def preview_letter(letter: str, max_chars: int = 500) -> None:\n",
    "    \"\"\"Show a preview of the letter with character count.\"\"\"\n",
    "    print(f\"Letter Preview (first {max_chars} chars of {len(letter)} total):\")\n",
    "    print(\"-\" * 60)\n",
    "    print(letter[:max_chars] + \"...\" if len(letter) > max_chars else letter)\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Simple Test Function\n",
    "\n",
    "def quick_test_session(client_letter: str = \"A\") -> None:\n",
    "    \"\"\"Quick test with pre-selected client.\"\"\"\n",
    "    case_folder = f\"data/case-files/{client_letter}\"\n",
    "    \n",
    "    try:\n",
    "        # Run the session\n",
    "        final_state = run_interactive_session(case_folder, \"annual_review\")\n",
    "        \n",
    "        print(f\"\\n✨ Quick test completed for Client {client_letter}\")\n",
    "        print(f\"📂 Results saved in: {final_state['session_dir']}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Test failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Template Tracking Utilities\n",
    "\n",
    "def browse_session_templates(session_name: str = None) -> None:\n",
    "    \"\"\"Browse templates from a specific session or let user choose.\"\"\"\n",
    "    sessions_dir = Path(\"sessions\")\n",
    "    \n",
    "    if not session_name:\n",
    "        # Show available sessions\n",
    "        print(\"\\n📁 AVAILABLE SESSIONS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        session_folders = sorted(sessions_dir.iterdir(), reverse=True)\n",
    "        valid_sessions = []\n",
    "        \n",
    "        for folder in session_folders:\n",
    "            if folder.is_dir() and (folder / \"templates\").exists():\n",
    "                valid_sessions.append(folder)\n",
    "        \n",
    "        if not valid_sessions:\n",
    "            print(\"No sessions with templates found.\")\n",
    "            return\n",
    "        \n",
    "        for i, folder in enumerate(valid_sessions[:10], 1):\n",
    "            print(f\"{i}. {folder.name}\")\n",
    "        \n",
    "        choice = input(\"\\nSelect session number: \").strip()\n",
    "        if not choice.isdigit() or int(choice) < 1 or int(choice) > len(valid_sessions):\n",
    "            print(\"Invalid choice.\")\n",
    "            return\n",
    "        \n",
    "        session_dir = valid_sessions[int(choice) - 1]\n",
    "    else:\n",
    "        session_dir = sessions_dir / session_name\n",
    "        if not session_dir.exists():\n",
    "            print(f\"Session not found: {session_name}\")\n",
    "            return\n",
    "    \n",
    "    # Browse templates in selected session\n",
    "    templates_dir = session_dir / \"templates\"\n",
    "    if not templates_dir.exists():\n",
    "        print(\"No templates found in this session.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n📂 TEMPLATES IN: {session_dir.name}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Group templates by type\n",
    "    general_templates = sorted(templates_dir.glob(\"iteration_*_general.md\"))\n",
    "    specific_templates = sorted(templates_dir.glob(\"iteration_*_specific.md\"))\n",
    "    \n",
    "    if general_templates:\n",
    "        print(\"\\n📄 General Templates:\")\n",
    "        for template in general_templates:\n",
    "            size = template.stat().st_size\n",
    "            print(f\"  - {template.name} ({size:,} bytes)\")\n",
    "    \n",
    "    if specific_templates:\n",
    "        print(\"\\n📄 Specific Templates:\")\n",
    "        for template in specific_templates:\n",
    "            size = template.stat().st_size\n",
    "            print(f\"  - {template.name} ({size:,} bytes)\")\n",
    "    \n",
    "    # Option to view specific template\n",
    "    print(\"\\nOptions:\")\n",
    "    print(\"1. View a specific template\")\n",
    "    print(\"2. Compare two templates\")\n",
    "    print(\"3. Export final templates\")\n",
    "    print(\"4. Return\")\n",
    "    \n",
    "    choice = input(\"\\nYour choice (1-4): \").strip()\n",
    "    \n",
    "    if choice == \"1\":\n",
    "        view_template_from_session(session_dir)\n",
    "    elif choice == \"2\":\n",
    "        compare_session_templates(session_dir)\n",
    "    elif choice == \"3\":\n",
    "        export_final_templates(session_dir)\n",
    "\n",
    "\n",
    "def view_template_from_session(session_dir: Path) -> None:\n",
    "    \"\"\"View a specific template from a session.\"\"\"\n",
    "    templates_dir = session_dir / \"templates\"\n",
    "    all_templates = sorted(templates_dir.glob(\"iteration_*.md\"))\n",
    "    \n",
    "    print(\"\\nAvailable templates:\")\n",
    "    for i, template in enumerate(all_templates, 1):\n",
    "        print(f\"{i}. {template.name}\")\n",
    "    \n",
    "    choice = input(\"\\nSelect template number: \").strip()\n",
    "    if choice.isdigit() and 1 <= int(choice) <= len(all_templates):\n",
    "        template_path = all_templates[int(choice) - 1]\n",
    "        print(f\"\\n📄 {template_path.name}\")\n",
    "        print(\"=\"*60)\n",
    "        with open(template_path, 'r') as f:\n",
    "            content = f.read()\n",
    "            # Show first 1000 chars\n",
    "            if len(content) > 1000:\n",
    "                print(content[:1000])\n",
    "                print(f\"\\n... (truncated, showing 1000/{len(content)} chars)\")\n",
    "            else:\n",
    "                print(content)\n",
    "\n",
    "\n",
    "def compare_session_templates(session_dir: Path) -> None:\n",
    "    \"\"\"Compare two template versions from a session.\"\"\"\n",
    "    templates_dir = session_dir / \"templates\"\n",
    "    \n",
    "    # Let user choose template type\n",
    "    print(\"\\nTemplate type to compare:\")\n",
    "    print(\"1. General templates\")\n",
    "    print(\"2. Specific templates\")\n",
    "    \n",
    "    type_choice = input(\"Your choice (1-2): \").strip()\n",
    "    if type_choice == \"1\":\n",
    "        template_type = \"general\"\n",
    "    elif type_choice == \"2\":\n",
    "        template_type = \"specific\"\n",
    "    else:\n",
    "        print(\"Invalid choice.\")\n",
    "        return\n",
    "    \n",
    "    # Find all versions of this template type\n",
    "    templates = sorted(templates_dir.glob(f\"iteration_*_{template_type}.md\"))\n",
    "    \n",
    "    if len(templates) < 2:\n",
    "        print(f\"Not enough {template_type} template versions to compare.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nAvailable {template_type} template versions:\")\n",
    "    for i, template in enumerate(templates, 1):\n",
    "        print(f\"{i}. {template.name}\")\n",
    "    \n",
    "    # Get first template\n",
    "    first_choice = input(\"\\nSelect first template number: \").strip()\n",
    "    if not first_choice.isdigit() or int(first_choice) < 1 or int(first_choice) > len(templates):\n",
    "        print(\"Invalid choice.\")\n",
    "        return\n",
    "    \n",
    "    # Get second template\n",
    "    second_choice = input(\"Select second template number: \").strip()\n",
    "    if not second_choice.isdigit() or int(second_choice) < 1 or int(second_choice) > len(templates):\n",
    "        print(\"Invalid choice.\")\n",
    "        return\n",
    "    \n",
    "    template1 = templates[int(first_choice) - 1]\n",
    "    template2 = templates[int(second_choice) - 1]\n",
    "    \n",
    "    with open(template1, 'r') as f:\n",
    "        content1 = f.read()\n",
    "    with open(template2, 'r') as f:\n",
    "        content2 = f.read()\n",
    "    \n",
    "    print(f\"\\n📊 Comparing {template1.name} vs {template2.name}\")\n",
    "    print(\"=\"*60)\n",
    "    print(compare_templates(content1, content2))\n",
    "\n",
    "\n",
    "def export_final_templates(session_dir: Path) -> None:\n",
    "    \"\"\"Export the final version of templates from a session.\"\"\"\n",
    "    templates_dir = session_dir / \"templates\"\n",
    "    \n",
    "    # Find the latest version of each template\n",
    "    general_templates = sorted(templates_dir.glob(\"iteration_*_general.md\"))\n",
    "    specific_templates = sorted(templates_dir.glob(\"iteration_*_specific.md\"))\n",
    "    \n",
    "    if not general_templates and not specific_templates:\n",
    "        print(\"No templates found to export.\")\n",
    "        return\n",
    "    \n",
    "    # Create export directory\n",
    "    export_dir = Path(\"exported_templates\")\n",
    "    export_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    client_name = session_dir.name.split('_')[0]\n",
    "    \n",
    "    exported = []\n",
    "    \n",
    "    if general_templates:\n",
    "        latest_general = general_templates[-1]\n",
    "        export_path = export_dir / f\"{client_name}_general_{timestamp}.md\"\n",
    "        with open(latest_general, 'r') as f_in:\n",
    "            with open(export_path, 'w') as f_out:\n",
    "                f_out.write(f_in.read())\n",
    "        exported.append(f\"General: {export_path.name}\")\n",
    "    \n",
    "    if specific_templates:\n",
    "        latest_specific = specific_templates[-1]\n",
    "        export_path = export_dir / f\"{client_name}_specific_{timestamp}.md\"\n",
    "        with open(latest_specific, 'r') as f_in:\n",
    "            with open(export_path, 'w') as f_out:\n",
    "                f_out.write(f_in.read())\n",
    "        exported.append(f\"Specific: {export_path.name}\")\n",
    "    \n",
    "    print(f\"\\n✅ Templates exported to {export_dir}:\")\n",
    "    for item in exported:\n",
    "        print(f\"  - {item}\")\n",
    "\n",
    "\n",
    "def get_template_evolution_report(session_name: str) -> None:\n",
    "    \"\"\"Generate a detailed report of how templates evolved in a session.\"\"\"\n",
    "    session_dir = Path(\"sessions\") / session_name\n",
    "    \n",
    "    if not session_dir.exists():\n",
    "        print(f\"Session not found: {session_name}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n📊 TEMPLATE EVOLUTION REPORT\")\n",
    "    print(f\"Session: {session_name}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Read all iteration files\n",
    "    iterations = []\n",
    "    for iter_file in sorted(session_dir.glob(\"iteration_*.json\")):\n",
    "        with open(iter_file, 'r') as f:\n",
    "            iterations.append(json.load(f))\n",
    "    \n",
    "    if not iterations:\n",
    "        print(\"No iteration data found.\")\n",
    "        return\n",
    "    \n",
    "    # Track template changes\n",
    "    print(\"\\n📈 Template Change Timeline:\")\n",
    "    for iter_data in iterations:\n",
    "        iter_num = iter_data['iteration']\n",
    "        print(f\"\\nIteration {iter_num}:\")\n",
    "        print(f\"  - Timestamp: {iter_data['timestamp']}\")\n",
    "        print(f\"  - Hallucinations: {iter_data['hallucinations_count']}\")\n",
    "        \n",
    "        if iter_data.get('template_changed_this_iteration', {}).get('general'):\n",
    "            print(f\"  - ✏️  General template updated\")\n",
    "        if iter_data.get('template_changed_this_iteration', {}).get('specific'):\n",
    "            print(f\"  - ✏️  Specific template updated\")\n",
    "        \n",
    "        if iter_data.get('user_feedback'):\n",
    "            print(f\"  - 💬 User feedback: {iter_data['user_feedback'][:50]}...\")\n",
    "        \n",
    "        if iter_data.get('user_decision'):\n",
    "            print(f\"  - 🎯 Decision: {iter_data['user_decision']}\")\n",
    "    \n",
    "    # Summary statistics\n",
    "    total_iterations = len(iterations)\n",
    "    general_changes = sum(1 for i in iterations if i.get('template_changed_this_iteration', {}).get('general'))\n",
    "    specific_changes = sum(1 for i in iterations if i.get('template_changed_this_iteration', {}).get('specific'))\n",
    "    \n",
    "    print(f\"\\n📊 Summary Statistics:\")\n",
    "    print(f\"  - Total iterations: {total_iterations}\")\n",
    "    print(f\"  - General template changes: {general_changes}\")\n",
    "    print(f\"  - Specific template changes: {specific_changes}\")\n",
    "    print(f\"  - Final hallucinations: {iterations[-1]['hallucinations_count'] if iterations else 0}\")\n",
    "\n",
    "\n",
    "# Quick access functions\n",
    "def list_recent_sessions_with_templates() -> None:\n",
    "    \"\"\"List recent sessions that have template changes.\"\"\"\n",
    "    sessions_dir = Path(\"sessions\")\n",
    "    \n",
    "    print(\"\\n📁 RECENT SESSIONS WITH TEMPLATE CHANGES\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    sessions_with_changes = []\n",
    "    \n",
    "    for session_dir in sorted(sessions_dir.iterdir(), reverse=True)[:20]:\n",
    "        if session_dir.is_dir():\n",
    "            # Check if templates were modified\n",
    "            summary_file = session_dir / \"session_summary.txt\"\n",
    "            if summary_file.exists():\n",
    "                with open(summary_file, 'r') as f:\n",
    "                    content = f.read()\n",
    "                    if \"Template Evolution:\" in content and \"Updated\" in content:\n",
    "                        sessions_with_changes.append(session_dir)\n",
    "    \n",
    "    if not sessions_with_changes:\n",
    "        print(\"No sessions with template changes found.\")\n",
    "        return\n",
    "    \n",
    "    for i, session in enumerate(sessions_with_changes[:10], 1):\n",
    "        print(f\"{i}. {session.name}\")\n",
    "        \n",
    "        # Show brief template change info\n",
    "        summary_file = session / \"session_summary.txt\"\n",
    "        with open(summary_file, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                if \"Updated\" in line and \"template\" in line:\n",
    "                    print(f\"   {line.strip()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Main Interactive Demo\n",
    "\n",
    "print(\"🚀 AI LETTER GENERATION - INTERACTIVE MODE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Choose a client\n",
    "print(\"\\nSelect a client:\")\n",
    "print(\"1. Client A\")\n",
    "print(\"2. Client B\") \n",
    "print(\"3. Client C\")\n",
    "print(\"4. Client E\")\n",
    "\n",
    "client_choice = input(\"\\nEnter choice (1-4): \").strip()\n",
    "\n",
    "if client_choice in [\"1\", \"2\", \"3\", \"4\"]:\n",
    "    # Map choice to client\n",
    "    client_map = {\n",
    "        \"1\": (\"A\", \"Client A\"),\n",
    "        \"2\": (\"B\", \"Client B\"),\n",
    "        \"3\": (\"C\", \"Client C\"),\n",
    "        \"4\": (\"E\", \"Client E\")\n",
    "    }\n",
    "    \n",
    "    folder_suffix, client_name = client_map[client_choice]\n",
    "    case_folder = f\"data/case-files/{folder_suffix}\"\n",
    "    \n",
    "    print(f\"\\n✅ Selected: {client_name}\")\n",
    "    \n",
    "    try:\n",
    "        # Run the interactive session - the workflow handles everything!\n",
    "        final_state = run_interactive_session(case_folder, \"annual_review\", max_iterations=5)\n",
    "        \n",
    "        print(\"\\n🎉 Session complete!\")\n",
    "        print(f\"📂 All files saved in: {final_state['session_dir']}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Error: {e}\")\n",
    "        print(\"\\nTroubleshooting:\")\n",
    "        print(\"• Check your GOOGLE_API_KEY in .env\")\n",
    "        print(\"• Verify case files exist in the folder\")\n",
    "        print(\"• Ensure all dependencies are installed\")\n",
    "        \n",
    "else:\n",
    "    print(\"❌ Invalid choice. Please run the cell again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Simple Test Demo\n",
    "# Run this cell to test the reworked chain execution\n",
    "\n",
    "print(\"🧪 TESTING REWORKED WORKFLOW\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# First test API\n",
    "print(\"\\n1️⃣ Testing API connection...\")\n",
    "if not test_api():\n",
    "    print(\"❌ API test failed! Please check your setup.\")\n",
    "else:\n",
    "    print(\"✅ API is working!\")\n",
    "    \n",
    "    # Quick workflow test\n",
    "    print(\"\\n2️⃣ Testing workflow components...\")\n",
    "    try:\n",
    "        # Test template reading\n",
    "        general = read_general_template()\n",
    "        specific = read_specific_template(\"annual_review\")\n",
    "        print(f\"✅ Templates loaded: general ({len(general)} chars), specific ({len(specific)} chars)\")\n",
    "        \n",
    "        # Test case reading\n",
    "        case_content = read_case_folder(\"data/case-files/A\")\n",
    "        print(f\"✅ Case content loaded: {len(case_content)} chars\")\n",
    "        \n",
    "        # Test case fact extraction\n",
    "        print(\"\\n3️⃣ Testing case fact extraction...\")\n",
    "        case_facts = extract_case_facts_once(case_content[:5000], \"data/case-files/A\")  # Use first 5000 chars for speed\n",
    "        print(f\"✅ Case facts extracted: {len(case_facts)} facts\")\n",
    "        if case_facts:\n",
    "            print(\"\\nSample facts:\")\n",
    "            for fact in case_facts[:3]:\n",
    "                print(f\"  - {fact.fact_type}: {fact.content[:80]}...\")\n",
    "        \n",
    "        # Test letter generation\n",
    "        print(\"\\n4️⃣ Testing letter generation...\")\n",
    "        test_letter = draft_letter(general, specific, case_content)\n",
    "        print(f\"✅ Letter generated: {len(test_letter)} chars\")\n",
    "        \n",
    "        # Test simple evaluation\n",
    "        print(\"\\n5️⃣ Testing simple evaluation...\")\n",
    "        evaluation = evaluate_letter(test_letter, case_content, general, specific)\n",
    "        print(f\"✅ Simple evaluation complete:\")\n",
    "        print(f\"   - Hallucinations: {len(evaluation.get('hallucinations', []))}\")\n",
    "        print(f\"   - Template needing work: {evaluation.get('template_needing_improvement', 'unknown')}\")\n",
    "        \n",
    "        # Test enhanced evaluation\n",
    "        if case_facts:\n",
    "            print(\"\\n6️⃣ Testing enhanced evaluation...\")\n",
    "            enhanced_eval = run_enhanced_evaluation(test_letter[:3000], case_facts)  # Use first 3000 chars for speed\n",
    "            print(f\"✅ Enhanced evaluation complete:\")\n",
    "            print(f\"   - Facts checked: {enhanced_eval.get('facts_checked', 0)}\")\n",
    "            print(f\"   - Score: {enhanced_eval.get('score', 'N/A')}%\")\n",
    "            print(f\"   - Hallucinations: {len(enhanced_eval.get('hallucinations', []))}\")\n",
    "        \n",
    "        print(\"\\n✅ ALL COMPONENTS WORKING!\")\n",
    "        print(\"\\n💡 You can now run Cell 11 for the full interactive experience\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Component test failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "letter-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
